{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from splinter import Browser\n",
    "from bs4 import BeautifulSoup\n",
    "from time import sleep\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setting up path to chrome driver\n",
    "executable_path={'executable_path':'chromedriver.exe'}\n",
    "\n",
    "#setting chrome as driver for splinter\n",
    "browser = Browser('chrome', **executable_path, headless=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#url with latest NASA stories\n",
    "url_1 = 'https://mars.nasa.gov/news/?page=0&per_page=40&order=publish_date+desc%2Ccreated_at+desc&search=&category=19%2C165%2C184%2C204&blank_scope=Latest'\n",
    "browser.visit(url_1)\n",
    "\n",
    "html = browser.html\n",
    "soup = BeautifulSoup(html, 'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get all news articles\n",
    "page_click_count=0\n",
    "\n",
    "while soup.find ('a', class_='button') and soup.find('ul', class_='item_list'):\n",
    "    #pressing the 'More' button as many times as the More button and news list appear\n",
    "    try:\n",
    "#           browser.find_by_xpath('//*[@id=\"page\"]/div[2]/div/article/div/section/div/footer/a').click()\n",
    "        button = browser.find_by_text('More')\n",
    "        if len(button) == 3:\n",
    "            button[1].click()\n",
    "            page_click_count+=1\n",
    "            sleep(2)\n",
    "        else:\n",
    "            break\n",
    "    except:\n",
    "        break\n",
    "\n",
    "#resetting html variable to browser that is fully loaded (all 'More' buttons pressed)\n",
    "html = browser.html\n",
    "soup = BeautifulSoup(html, 'lxml')\n",
    "\n",
    "#parsing through html to get date, title, and summary of all news articles on url page\n",
    "results=soup.find_all('li', class_='slide')\n",
    "pre_df=[]\n",
    "for result in results:\n",
    "    date=result.find('div', class_='list_date').text\n",
    "    title=result.find('div', class_='content_title').text\n",
    "    text=result.find('div', class_='article_teaser_body').text\n",
    "    pre_df.append([date, title, text])\n",
    "\n",
    "#close browser\n",
    "browser.quit()\n",
    "\n",
    "news_df=pd.DataFrame(pre_df)\n",
    "news_df.columns=('Date', 'Title', 'Summary')\n",
    "news_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
